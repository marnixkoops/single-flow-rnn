{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import mlflow\n",
    "from ml_metrics import average_precision\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# 🚀 EXPERIMENT SETTINGS\n",
    "####################################################################################################\n",
    "\n",
    "# run settings\n",
    "DRY_RUN = False  # runs flow on small subset of data for speed and disables mlfow tracking\n",
    "WEEKS_OF_DATA = 2  # load 1,2 or 3 weeks of data (currently in production it is 1 week)\n",
    "\n",
    "# define where we run and on which device (GPU/CPU)\n",
    "# GPU_AVAILABLE = tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)\n",
    "all_devices = str(device_lib.list_local_devices())\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if \"Tesla P100\" in all_devices:\n",
    "    DEVICE = \"Tesla P100 GPU\"\n",
    "    MACHINE = \"cloud\"\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)  # no allocating memory upfront\n",
    "elif \"GPU\" in all_devices:\n",
    "    DEVICE = \"GPU\"\n",
    "    MACHINE = \"cloud\"\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "elif \"CPU\" in all_devices:\n",
    "    DEVICE = \"CPU\"\n",
    "    MACHINE = \"local\"\n",
    "\n",
    "print(\"🧠 Running TensorFlow version {} on {}\".format(tf.__version__, DEVICE))\n",
    "\n",
    "# input data\n",
    "DATA_PATH1 = \"marnix-single-flow-rnn/data/ga_product_sequence_20191013.csv\"\n",
    "DATA_PATH2 = \"marnix-single-flow-rnn/data/ga_product_sequence_20191020.csv\"\n",
    "DATA_PATH3 = \"marnix-single-flow-rnn/data/ga_product_sequence_20191027.csv\"\n",
    "DATA_PATH4 = \"marnix-single-flow-rnn/data/ga_product_sequence_20191103.csv\"\n",
    "\n",
    "# data constants\n",
    "N_TOP_PRODUCTS = 15000  # note, 6000 is ~70% views, 8000 ~80%, 10000 ~84%, 12000 ~87%, 15000 ~90%\n",
    "MIN_PRODUCTS_TRAIN = 3  # sequences with less products are considered invalid and removed\n",
    "MIN_PRODUCTS_TEST = 2  # sequences with less products are considered invalid and removed\n",
    "WINDOW_LEN = 5  # fixed moving window size for generating input-sequence/target rows for training\n",
    "PRED_LOOKBACK = 5  # number of most recent products used per sequence in the test set to predict on\n",
    "TOP_K_OUTPUT_LEN = 10  # number of top K product recommendations to extract from the model\n",
    "\n",
    "# model constants\n",
    "EMBED_DIM = 48  # number of dimensions for the embeddings\n",
    "N_HIDDEN_UNITS = 192  # number of units in the GRU layers\n",
    "MAX_EPOCHS = 12  # maximum number of epochs to train for\n",
    "BATCH_SIZE = 1024  # batch size for training\n",
    "DROPOUT = 0.25  # input data dropout\n",
    "RECURRENT_DROPOUT = 0.25  # hidden state dropout in the GRU during training\n",
    "LEARNING_RATE = 0.002\n",
    "OPTIMIZER = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=LEARNING_RATE\n",
    ")  # note, tested a couple (RMSProp, Adam, Nadam), Nadam seems fast with good results\n",
    "\n",
    "# training constants\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15  # note, creates a gap in time between train-test, no val improves performance\n",
    "TEST_RATIO = 0.15  # note, this % results in more samples when more weeks of data are used\n",
    "SHUFFLE_TRAIN_SET = True  # shuffles the training sequences (row-wise), seems smart for training\n",
    "\n",
    "# dry run constants for development and debugging\n",
    "if DRY_RUN:\n",
    "    SEQUENCES = 100000\n",
    "    N_TOP_PRODUCTS = 1000\n",
    "    EMBED_DIM = 32\n",
    "    N_HIDDEN_UNITS = 64\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_EPOCHS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# 🚀 INPUT DATA\n",
    "####################################################################################################\n",
    "\n",
    "print(\"\\n🚀 Starting experiment on {}\".format(datetime.datetime.now() + datetime.timedelta(hours=1)))\n",
    "print(\"     Using DRY_RUN: {} and {} weeks of data\".format(DRY_RUN, WEEKS_OF_DATA))\n",
    "print(\"     Reading raw input sequence data from disk\")\n",
    "\n",
    "if DRY_RUN:\n",
    "    sequence_df = pd.read_csv(DATA_PATH3)\n",
    "    sequence_df = sequence_df.tail(SEQUENCES).copy()  # take a small subset of data for debugging\n",
    "elif WEEKS_OF_DATA == 2:\n",
    "    sequence_df = pd.read_csv(DATA_PATH2)\n",
    "    sequence_df2 = pd.read_csv(DATA_PATH3)\n",
    "    sequence_df = sequence_df.append(sequence_df2)\n",
    "    del sequence_df2\n",
    "elif WEEKS_OF_DATA == 3:\n",
    "    sequence_df = pd.read_csv(DATA_PATH1)\n",
    "    sequence_df2 = pd.read_csv(DATA_PATH2)\n",
    "    sequence_df3 = pd.read_csv(DATA_PATH3)\n",
    "    sequence_df = sequence_df.append(sequence_df2).append(sequence_df3)\n",
    "    del sequence_df2, sequence_df3\n",
    "elif WEEKS_OF_DATA == 4:\n",
    "    sequence_df = pd.read_csv(DATA_PATH1)\n",
    "    sequence_df2 = pd.read_csv(DATA_PATH2)\n",
    "    sequence_df3 = pd.read_csv(DATA_PATH3)\n",
    "    sequence_df4 = pd.read_csv(DATA_PATH4)\n",
    "    sequence_df = sequence_df.append(sequence_df2).append(sequence_df3).append(sequence_df4)\n",
    "    del sequence_df2, sequence_df3, sequence_df4\n",
    "else:\n",
    "    sequence_df = pd.read_csv(DATA_PATH1)\n",
    "\n",
    "MIN_DATE, MAX_DATE = sequence_df[\"visit_date\"].min(), sequence_df[\"visit_date\"].max()\n",
    "print(\"     Data contains {} sequences from {} to {}\".format(len(sequence_df), MIN_DATE, MAX_DATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# 🚀 PREPARE DATA FOR MODELING\n",
    "####################################################################################################\n",
    "\n",
    "t_prep = time.time()  # start timer for preparing data\n",
    "\n",
    "\n",
    "def print_memory_footprint(array):\n",
    "    \"\"\"Prints a statement with the memory size of the input array\"\"\"\n",
    "    print(\"     Memory footprint of array: {:.4} MegaBytes\".format(array.nbytes * 1e-6))\n",
    "\n",
    "\n",
    "print(\"\\n💾 Processing data\")\n",
    "print(\"     Tokenizing, padding, filtering & splitting sequences\")\n",
    "# define tokenizer to encode sequences and include N most popular items (occurence)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=N_TOP_PRODUCTS)\n",
    "tokenizer.fit_on_texts(sequence_df[\"product_sequence\"])  # encode string sequences as tokens\n",
    "sequences = tokenizer.texts_to_sequences(sequence_df[\"product_sequence\"])  # array of sequences\n",
    "del sequence_df\n",
    "gc.collect()\n",
    "\n",
    "# pre-pad sequences with 0's, length is based on longest present sequence\n",
    "# this is required to transform the variable length sequences into equal train-test pairs\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding=\"pre\")\n",
    "\n",
    "if N_TOP_PRODUCTS is None:\n",
    "    N_TOP_PRODUCTS = len(tokenizer.word_index) + 1\n",
    "    print(\"     Included ALL products present in the input data ({})\".format(N_TOP_PRODUCTS))\n",
    "else:\n",
    "    print(\"     Included top {} most popular products\".format(N_TOP_PRODUCTS))\n",
    "\n",
    "print_memory_footprint(padded_sequences)\n",
    "\n",
    "del sequences\n",
    "gc.collect()\n",
    "\n",
    "# split into train/test subsets before reshaping sequence for training/validation\n",
    "# (since we subsample longer sequences of a single customer into multiple train/validation pairs,\n",
    "# while we do not wish to predict on multiple sequences of a single customer\n",
    "test_index = int(TEST_RATIO * len(padded_sequences))\n",
    "padded_sequences_train = padded_sequences[test_index:].copy()\n",
    "padded_sequences_test = padded_sequences[:test_index].copy()\n",
    "\n",
    "\n",
    "def filter_valid_sequences(array, min_items=MIN_PRODUCTS_TRAIN):\n",
    "    \"\"\"Filters sequences that are not valid. Invalid sequences do not contain enough products or end\n",
    "    in a 0 (padding) which can occur due to creating subsequences of longer sequences for training.\n",
    "    Args:\n",
    "        array (array): Input matrix with sequences.\n",
    "        min_items (int): Treshold for filtering invalid sequences.\n",
    "    Returns:\n",
    "        array: Valid sequences\n",
    "    \"\"\"\n",
    "    pre_len = len(array)\n",
    "    array = array[array[:, -1] != 0]  # ending in 0 is a duplicate subsequence or empty sequence\n",
    "    min_product_mask = np.sum((array != 0), axis=1) >= min_items  # create mask for min products\n",
    "    valid_sequences = array[min_product_mask].copy()\n",
    "    print(\"     Removed {} invalid sequences\".format(pre_len - len(valid_sequences)))\n",
    "    print(\"     Kept {} valid sequences\".format(len(valid_sequences)))\n",
    "    return valid_sequences\n",
    "\n",
    "\n",
    "# Untested idea to remove sequences that only contain a single unique product (no information)\n",
    "# def filter_repeated_unique_product_sequences(array):\n",
    "#     \"\"\"Checks if the last product is equal to all other products in the sequence. The last product\n",
    "#     is considered as the first product token can be a 0 due to pre-padding.\n",
    "#     Args:\n",
    "#         array (type): Description of parameter `array`.\n",
    "#     Returns:\n",
    "#         (type): Description of returned object.\n",
    "#     \"\"\"\n",
    "#     repeated_product_mask = np.all(array[ , -1] != array[ , :])\n",
    "#     return array[repeated_product_mask].copy()\n",
    "\n",
    "\n",
    "padded_sequences_train = filter_valid_sequences(\n",
    "    padded_sequences_train, min_items=MIN_PRODUCTS_TRAIN\n",
    ")\n",
    "padded_sequences_test = filter_valid_sequences(padded_sequences_test, min_items=MIN_PRODUCTS_TEST)\n",
    "\n",
    "print(\"\\n     Training & evaluating model on {} sequences\".format(len(padded_sequences_train)))\n",
    "print(\"     Testing recommendations on {} sequences\".format(len(padded_sequences_test)))\n",
    "print_memory_footprint(padded_sequences_train)\n",
    "print_memory_footprint(padded_sequences_test)\n",
    "\n",
    "# clean up memory\n",
    "del padded_sequences\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# generate subsequences from longer sequences with a moving window for model training\n",
    "# this numpy function is fast but a bit tricky, be sure to validate output when changing stuff\n",
    "def generate_train_test_pairs(array, input_length=WINDOW_LEN, step_size=1):\n",
    "    \"\"\"Creates multiple subsequences out of longer sequences in the matrix to be used for training.\n",
    "    Output shape is based on the input_length. Note that the output width is input_length + 1 since\n",
    "    we later take the last column of the matrix to obtain an input matrix of width input_length and\n",
    "    a vector with corresponding targets (next item in that sequence).\n",
    "    Args:\n",
    "        array (array): Input matrix with equal length padded sequences.\n",
    "        input_length (int): Size of sliding window, equal to desired length of input sequence.\n",
    "        step_size (int): Can be used to skip items in the sequence.\n",
    "    Returns:\n",
    "        array: Reshaped matrix with # columns equal to input_length + 1 (input + target item).\n",
    "    \"\"\"\n",
    "    shape = (array.size - input_length + 2, input_length + 1)\n",
    "    strides = array.strides * 2\n",
    "    window = np.lib.stride_tricks.as_strided(array, strides=strides, shape=shape)[0::step_size]\n",
    "    return window.copy()\n",
    "\n",
    "\n",
    "# generate sliding window of sequences with x=WINDOW_LEN input products and y=1 target product\n",
    "print(\"\\n     Reshaping into train-test subsequences with fixed window size for training\")\n",
    "padded_sequences_train = np.apply_along_axis(generate_train_test_pairs, 1, padded_sequences_train)\n",
    "padded_sequences_train = np.vstack(padded_sequences_train).copy()  # stack sequences\n",
    "print(\"     Generated {} subsequences for training/validation\".format(len(padded_sequences_train)))\n",
    "\n",
    "# filter sequences, note that due to reshaping invalid sequences can be re-introduced\n",
    "padded_sequences_train = filter_valid_sequences(\n",
    "    padded_sequences_train, min_items=MIN_PRODUCTS_TRAIN\n",
    ")\n",
    "print_memory_footprint(padded_sequences_train)\n",
    "\n",
    "# shuffle training sequences randomly (across rows, not within sequences ofcourse)\n",
    "if SHUFFLE_TRAIN_SET:\n",
    "    np.random.shuffle(padded_sequences_train)  # operates in-place\n",
    "\n",
    "# split sequences into subsets for training/validation/testing\n",
    "# the last column of each row is the target product for each input subsequence\n",
    "val_index = int(VAL_RATIO * len(padded_sequences_train))\n",
    "X_train, y_train = padded_sequences_train[:-val_index, :-1], padded_sequences_train[:-val_index, -1]\n",
    "X_val, y_val = padded_sequences_train[:val_index, :-1], padded_sequences_train[:val_index, -1]\n",
    "X_test, y_test = padded_sequences_test[:, -(PRED_LOOKBACK + 1) : -1], padded_sequences_test[:, -1]\n",
    "\n",
    "# only train-test split (no validation), untested but this should improve MAP due to having no gap\n",
    "# X_train, y_train = padded_sequences_train[:, :-1], padded_sequences_train[:, -1]\n",
    "# X_test, y_test = padded_sequences_test[:, -5:-1], padded_sequences_test[:, -1]\n",
    "\n",
    "print(\"\\n     Dropping some remainder rows to fit data into batches of {}\".format(BATCH_SIZE))\n",
    "train_index = len(X_train) - len(X_train) % BATCH_SIZE\n",
    "val_index = len(X_val) - len(X_val) % BATCH_SIZE\n",
    "test_index = len(X_test) - len(X_test) % BATCH_SIZE\n",
    "X_train, y_train = X_train[:train_index, :], y_train[:train_index]\n",
    "X_val, y_val = X_val[:val_index:, :], y_val[:val_index]\n",
    "X_test, y_test = X_test[:test_index, :], y_test[:test_index]\n",
    "\n",
    "print(\"     Final dataset dimensions:\")\n",
    "print(\"     Training X {}, y {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"     Validation X {}, y {}\".format(X_val.shape, y_val.shape))\n",
    "print(\"     Testing X {}, y {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "print(\"⏱️ Elapsed time for processing input data: {:.3} seconds\".format(time.time() - t_prep))\n",
    "\n",
    "del padded_sequences_train, padded_sequences_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# 🚀 DEFINE AND TRAIN RECURRENT NEURAL NETWORK\n",
    "####################################################################################################\n",
    "\n",
    "t_train = time.time()  # start timer for training\n",
    "\n",
    "print(\"\\n🧠 Defining network\")\n",
    "tf.keras.backend.clear_session()  # clear potentially remaining network graphs in the memory\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def embedding_GRU_model(\n",
    "    vocab_size=N_TOP_PRODUCTS,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_units=N_HIDDEN_UNITS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    dropout=DROPOUT,\n",
    "    recurrent_dropout=RECURRENT_DROPOUT,\n",
    "):\n",
    "    \"\"\"Defines a RNN model with a trainable embedding input layer and GRU units.\n",
    "    Args:\n",
    "        vocab_size (int): Number of unique products included in the data.\n",
    "        embed_dim (int): Number of embedding dimensions.\n",
    "        num_units (int): Number of units for the GRU layer\n",
    "        batch_size (int): Number of subsequences used in a single pass during training.\n",
    "        dropout (float): Probability of dropping an input subsequence.\n",
    "        recurrent_dropout (float): Probability of dropping a hidden state during training.\n",
    "\n",
    "    Returns:\n",
    "        tensorflow.keras.Sequential: Model object\n",
    "\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Embedding(\n",
    "                input_dim=N_TOP_PRODUCTS,\n",
    "                output_dim=EMBED_DIM,\n",
    "                batch_input_shape=[BATCH_SIZE, None],\n",
    "                mask_zero=True,\n",
    "            ),\n",
    "            tf.keras.layers.GRU(\n",
    "                units=N_HIDDEN_UNITS,\n",
    "                dropout=DROPOUT,\n",
    "                recurrent_dropout=RECURRENT_DROPOUT,\n",
    "                return_sequences=False,\n",
    "                stateful=True,\n",
    "                recurrent_initializer=\"glorot_uniform\",\n",
    "                recurrent_activation=\"sigmoid\",  # required for CuDNN GPU support\n",
    "                reset_after=True,  # required for CuDNN GPU support\n",
    "            ),\n",
    "            tf.keras.layers.Dense(N_TOP_PRODUCTS, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = embedding_GRU_model(\n",
    "    vocab_size=N_TOP_PRODUCTS, embed_dim=EMBED_DIM, num_units=N_HIDDEN_UNITS, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# network info\n",
    "print(\"\\n     Network summary: \\n{}\".format(model.summary()))\n",
    "total_params = model.count_params()\n",
    "# model.get_config() # highly detailed model parameter settings\n",
    "\n",
    "# early stopping monitor, stops training if no improvement in validation set for 1 epochs\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=1, verbose=1, restore_best_weights=False\n",
    ")\n",
    "\n",
    "print(\"     Training for a maximum of {} Epochs with batch size {}\".format(MAX_EPOCHS, BATCH_SIZE))\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    callbacks=[early_stopping_monitor],\n",
    ")\n",
    "\n",
    "train_time = time.time() - t_train\n",
    "print(\n",
    "    \"⏱️ Elapsed time for training network with {} parameters on {} sequences: {:.3} minutes\".format(\n",
    "        total_params, len(y_train), train_time / 60\n",
    "    )\n",
    ")\n",
    "\n",
    "del X_val, y_val, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🧠 Evaluating recommendations of network\")\n",
    "t_pred = time.time()  # start timer for predictions\n",
    "print(\"     Creating recommendations on test set\")\n",
    "\n",
    "\n",
    "def generate_predicted_sequences(y_pred_probs, output_length=TOP_K_OUTPUT_LEN):\n",
    "    \"\"\"Function to extract predicted output sequences. Output is based on the predicted logit values\n",
    "    where the highest probability corresponds to the first recommended item and so forth.\n",
    "    Output positions are based on probability from high to low so the output sequence is ordered.\n",
    "    To be used for obtaining multiple product recommendations and calculating MAP@K values.\n",
    "    Args:\n",
    "        y_pred_probs (array): Predicted probabilities for all included products.\n",
    "        output_length (int): Number of top K products to extract from the prediction array.\n",
    "    Returns:\n",
    "        array: Product recommendation matrix with shape [X_test, output_length]\n",
    "    \"\"\"\n",
    "    # obtain indices of highest logit values, the position corresponds to the encoded item\n",
    "    ind_of_max_logits = np.argpartition(y_pred_probs, -output_length)[-output_length:]\n",
    "    # order the sequence, sorting the negative values ascending equals sorting descending\n",
    "    ordered_predicted_sequences = ind_of_max_logits[np.argsort(-y_pred_probs[ind_of_max_logits])]\n",
    "\n",
    "    return ordered_predicted_sequences\n",
    "\n",
    "\n",
    "# model.predict occasionaly leads to memory leaking issues when input data is large (10K+ products).\n",
    "# issue is known and should be fixed soon: https://github.com/keras-team/keras/issues/13118 and\n",
    "# https://github.com/tensorflow/tensorflow/issues/33009\n",
    "# y_pred_probs = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "# predict in two stages to fit in GPU memory (array is 4 bytes * sequences * products = big)\n",
    "dividing_partition_row = len(X_test) // 2\n",
    "remainder_due_to_batch_size = dividing_partition_row % BATCH_SIZE  # needs to fit into BATCH_SIZE\n",
    "dividing_partition_row = dividing_partition_row - remainder_due_to_batch_size\n",
    "\n",
    "predicted_sequences = np.empty(\n",
    "    [len(X_test), TOP_K_OUTPUT_LEN], dtype=np.float32\n",
    ")  # pre-allocate required memory for array for efficiency\n",
    "\n",
    "# first partition of recomendations\n",
    "y_pred_probs = model.predict(X_test[:dividing_partition_row])\n",
    "predicted_sequences[:dividing_partition_row] = np.apply_along_axis(\n",
    "    generate_predicted_sequences, 1, y_pred_probs  # extract TOP_K_OUTPUT_LEN recommendations\n",
    ")\n",
    "del y_pred_probs\n",
    "gc.collect()\n",
    "\n",
    "# second partition of recomendations\n",
    "y_pred_probs = model.predict(X_test[dividing_partition_row:])\n",
    "predicted_sequences[dividing_partition_row:] = np.apply_along_axis(\n",
    "    generate_predicted_sequences, 1, y_pred_probs  # extract TOP_K_OUTPUT_LEN recommendations\n",
    ")\n",
    "del y_pred_probs\n",
    "gc.collect()\n",
    "\n",
    "# # custom batched prediction loop to avoid memory leak issues in the model.predict call\n",
    "# y_pred_probs = np.empty(\n",
    "#     [len(X_test), N_TOP_PRODUCTS], dtype=np.float32\n",
    "# )  # pre-allocate required memory for array for efficiency (4 bytes * sequences * products = big)\n",
    "#\n",
    "# BATCH_INDICES = np.arange(start=0, stop=len(X_test), step=BATCH_SIZE)  # row indices of batches\n",
    "# BATCH_INDICES = np.append(BATCH_INDICES, len(X_test))  # add final batch_end row\n",
    "#\n",
    "# for index in np.arange(len(BATCH_INDICES) - 1):\n",
    "#     batch_start = BATCH_INDICES[index]  # first row of the batch\n",
    "#     batch_end = BATCH_INDICES[index + 1]  # last row of the batch\n",
    "#     y_pred_probs[batch_start:batch_end] = model.predict_on_batch(X_test[batch_start:batch_end])\n",
    "\n",
    "pred_time = time.time() - t_pred\n",
    "print(\n",
    "    \"⏱️  Elapsed time for predicting {} odds times {} sequences: {:.3} seconds\".format(\n",
    "        N_TOP_PRODUCTS, len(y_test), pred_time\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# 🚀 EVALUATE RECOMMENDATIONS\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "def extract_overlap_per_sequence(X_test, y_pred):\n",
    "    \"\"\"Finds overlapping items that are present in both arrays per row.\n",
    "    Args:\n",
    "        X_test (array): Input sequences for testing.\n",
    "        y_pred (array): Predicted output sequences.\n",
    "    Returns:\n",
    "        list: A list of overlapping products for each row\n",
    "    \"\"\"\n",
    "    overlap_items = [set(X_test[row, -5:]) & set(y_pred[row, :5]) for row in range(len(X_test))]\n",
    "    return overlap_items\n",
    "\n",
    "\n",
    "def compute_average_novelty(X_test, y_pred):\n",
    "    \"\"\"Computes the average overlap over all input and predicted sequences. Note that novelty is\n",
    "    computed as 1 - overlap as the new items are the ones that are not present in both arrays.\n",
    "    Args:\n",
    "        X_test (array): Input sequences for testing.\n",
    "        y_pred (array): Predicted output sequences.\n",
    "    Returns:\n",
    "        type: Average novelty over all predictions.\n",
    "    \"\"\"\n",
    "    overlap_items = extract_overlap_per_sequence(X_test, y_pred)\n",
    "    overlap_sum = np.sum([len(overlap_items[row]) for row in range(len(overlap_items))])\n",
    "    average_novelty = 1 - (\n",
    "        overlap_sum / (len(X_test) * X_test.shape[1])\n",
    "    )  # new items are the ones that do not overlap\n",
    "    return average_novelty\n",
    "\n",
    "\n",
    "print(\"\\n     Performance metrics on test set:\")\n",
    "y_pred = np.vstack(predicted_sequences[:, 0])  # top 1 recommendation (predicted next click)\n",
    "gc.collect()\n",
    "\n",
    "# TODO this ml_metric and vstack stuff can be implemented faster\n",
    "accuracy = np.round(accuracy_score(y_test, y_pred), 4)\n",
    "y_test = np.vstack(y_test)\n",
    "map3 = np.round(average_precision.mapk(y_test, predicted_sequences, k=3), 4)\n",
    "map5 = np.round(average_precision.mapk(y_test, predicted_sequences, k=5), 4)\n",
    "map10 = np.round(average_precision.mapk(y_test, predicted_sequences, k=10), 4)\n",
    "coverage = np.round(len(np.unique(predicted_sequences[:, :5])) / len(np.unique(X_train)), 4)\n",
    "novelty = np.round(compute_average_novelty(X_test[:, -5:], predicted_sequences[:, :5]), 4)\n",
    "\n",
    "print(\"\\n    Embedding GRU-RNN:\")\n",
    "print(\"     Accuracy @ 1   {:.4}%\".format(accuracy * 100))\n",
    "print(\"     MAP @ 3        {:.4}%\".format(map3 * 100))\n",
    "print(\"     MAP @ 5        {:.4}%\".format(map5 * 100))\n",
    "print(\"     MAP @ 10       {:.4}%\".format(map10 * 100))\n",
    "print(\"     Coverage       {:.4}%\".format(coverage * 100))\n",
    "print(\"     Novelty        {:.4}%\".format(novelty * 100))\n",
    "\n",
    "print(\"\\n    Baseline Metrics:\")\n",
    "print(\"    Top 5 Most Popular:\")\n",
    "\n",
    "pop_products = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # encoding is based on occurence, 1 is most frequent\n",
    "pop_products = np.repeat([pop_products], axis=0, repeats=len(y_test))\n",
    "accuracy_pop = np.round(accuracy_score(y_test, pop_products[:, -1:]), 4)\n",
    "map3_pop = np.round(average_precision.mapk(y_test, pop_products, k=3), 4)\n",
    "map5_pop = np.round(average_precision.mapk(y_test, pop_products, k=5), 4)\n",
    "map10_pop = np.round(average_precision.mapk(y_test, pop_products, k=10), 4)\n",
    "coverage_pop = np.round(len(np.unique(pop_products[:, :5])) / len(np.unique(X_train)), 4)\n",
    "novelty_pop = np.round(compute_average_novelty(X_test[:, -5:], pop_products[:, :5]), 4)\n",
    "\n",
    "print(\"     Accuracy @ 1   {:.4}%\".format(accuracy_pop * 100))\n",
    "print(\"     MAP @ 3        {:.4}%\".format(map3_pop * 100))\n",
    "print(\"     MAP @ 5        {:.4}%\".format(map5_pop * 100))\n",
    "print(\"     MAP @ 10       {:.4}%\".format(map10_pop * 100))\n",
    "print(\"     Coverage       {:.4}%\".format(coverage_pop * 100))\n",
    "print(\"     Novelty        {:.4}%\".format(novelty_pop * 100))\n",
    "\n",
    "print(\"\\n    Last 5 Views:\")\n",
    "\n",
    "accuracy_views = np.round(accuracy_score(y_test, X_test[:, -1:]), 4)\n",
    "map3_views = np.round(average_precision.mapk(y_test, X_test[:, -3:], k=3), 4)\n",
    "map5_views = np.round(average_precision.mapk(y_test, X_test[:, -5:], k=5), 4)\n",
    "map10_views = np.round(average_precision.mapk(y_test, X_test[:, -10:], k=10), 4)\n",
    "coverage_views = np.round(len(np.unique(X_test[:, -5:])) / len(np.unique(X_train)), 4)\n",
    "novelty_views = np.round(compute_average_novelty(X_test, X_test[:, -5:]), 4)\n",
    "\n",
    "print(\"     Accuracy @ 1   {:.4}%\".format(accuracy_views * 100))\n",
    "print(\"     MAP @ 3        {:.4}%\".format(map3_views * 100))\n",
    "print(\"     MAP @ 5        {:.4}%\".format(map5_views * 100))\n",
    "print(\"     MAP @ 10       {:.4}%\".format(map10_views * 100))\n",
    "print(\"     Coverage       {:.4}%\".format(coverage_views * 100))\n",
    "print(\"     Novelty        {:.4}%\".format(novelty_views * 100))\n",
    "\n",
    "# plot model training history results\n",
    "hist_dict = model_history.history\n",
    "train_loss_values = hist_dict[\"loss\"]\n",
    "train_acc_values = hist_dict[\"accuracy\"]\n",
    "val_loss_values = hist_dict[\"val_loss\"]\n",
    "val_acc_values = hist_dict[\"val_accuracy\"]\n",
    "epochs = np.arange(1, len(model_history.history[\"loss\"]) + 1).astype(int)\n",
    "\n",
    "plt.close()\n",
    "validation_plots, ax = plt.subplots(2, 1, figsize=(10, 6))\n",
    "plt.subplot(211)  # plot loss over epochs\n",
    "plt.plot(\n",
    "    epochs, train_loss_values, \"deepskyblue\", linestyle=\"dashed\", marker=\"o\", label=\"Train Loss\"\n",
    ")\n",
    "plt.plot(epochs, val_loss_values, \"springgreen\", marker=\"o\", label=\"Val Loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"{} loss over Epochs\".format(model.loss).upper(), size=13, weight=\"bold\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(212)  # plot accuracy over epochs\n",
    "plt.plot(\n",
    "    epochs, train_acc_values, \"deepskyblue\", linestyle=\"dashed\", marker=\"o\", label=\"Train Accuracy\"\n",
    ")\n",
    "plt.plot(epochs, val_acc_values, \"springgreen\", marker=\"o\", label=\"Val Accuracy\")\n",
    "plt.plot(epochs[-1], accuracy, \"#16a085\", marker=\"8\", markersize=12, label=\"Test Accuracy\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy (k=1) over Epochs\".format(model.loss).upper(), size=13, weight=\"bold\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"marnix-single-flow-rnn/plots/validation_plots.png\")\n",
    "\n",
    "print(\"✅ All done, total elapsed time: {:.3} minutes\".format((time.time() - t_prep) / 60))\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
